{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["This file represents our implementation of a Semantic Analysis model using Scikit-learn's Random Forest model. We use this model to determine positive or negative sentiment of Yelp reviews.\n","\n","This implementation is heavily based on the Semantic Analysis model implementation in this Github repository: https://github.com/asathiya007/nba-trending-teams"],"metadata":{"id":"9TFkOtM0AKx3"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"8wFutLvKOjcv"},"outputs":[],"source":["import joblib\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.stem.snowball import SnowballStemmer\n","import pandas as pd\n","import numpy as np\n","import pickle\n","import re\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n","from sklearn.model_selection import train_test_split\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import f1_score, accuracy_score\n","import statistics \n","import time "]},{"cell_type":"code","source":["#Kaggle direct access\n","! pip install -q kaggle\n","from google.colab import files\n","files.upload()\n","! mkdir ~/.kaggle\n","! cp kaggle.json ~/.kaggle/\n","! chmod 600 ~/.kaggle/kaggle.json\n","! kaggle datasets list\n","\n","#Downloads file online\n","!kaggle datasets download -d yelp-dataset/yelp-dataset\n","!unzip yelp-dataset.zip"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":665},"id":"thplgd_-ZhZl","executionInfo":{"status":"ok","timestamp":1670533045283,"user_tz":300,"elapsed":242364,"user":{"displayName":"Preethi Narayan","userId":"06220921649826715175"}},"outputId":"c8823cbe-a545-4210-fbbb-7d03c2711a46"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-e7e6b748-888e-45f3-9340-452dd40079e1\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-e7e6b748-888e-45f3-9340-452dd40079e1\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving kaggle.json to kaggle.json\n","ref                                                             title                                                size  lastUpdated          downloadCount  voteCount  usabilityRating  \n","--------------------------------------------------------------  --------------------------------------------------  -----  -------------------  -------------  ---------  ---------------  \n","meirnizri/covid19-dataset                                       COVID-19 Dataset                                      5MB  2022-11-13 15:47:17           6607        195  1.0              \n","madhurpant/world-deaths-and-causes-1990-2019                    World Deaths and Causes (1990 - 2019)               442KB  2022-11-29 07:09:27           1214         30  1.0              \n","thedevastator/jobs-dataset-from-glassdoor                       Salary Prediction                                     3MB  2022-11-16 13:52:31           4358        100  1.0              \n","thedevastator/how-much-sleep-do-americans-really-get            How Much Sleep Do Americans Really Get?               8KB  2022-11-25 09:13:12            824         31  1.0              \n","akshaydattatraykhare/data-for-admission-in-the-university       Data for Admission in the University                  4KB  2022-10-27 11:05:45           8089        175  1.0              \n","aguado/bike-rental-data-set-uci                                 Bike Rental Data Set - UCI                          124KB  2022-11-30 10:31:04            766         27  1.0              \n","dbarteaux99/stable-diffusion-1-5                                Stable Diffusion 1.5 (normal and EMAonly) with vae    7GB  2022-10-23 15:40:29            121         20  0.9375           \n","whenamancodes/predict-diabities                                 Predict Diabetes                                      9KB  2022-11-09 12:18:49           4630         87  1.0              \n","die9origephit/fifa-world-cup-2022-complete-dataset              Fifa World Cup 2022: Complete Dataset                 7KB  2022-12-07 11:22:54            500         39  0.9411765        \n","prosperchuks/health-dataset                                     Diabetes, Hypertension and Stroke Prediction        597KB  2022-11-23 10:04:03           1937         47  1.0              \n","mvieira101/global-cost-of-living                                Global Cost of Living                                 1MB  2022-12-03 16:37:53           1406         40  0.9705882        \n","thedevastator/cancer-patients-and-air-pollution-a-new-link       Lung Cancer Prediction                               7KB  2022-11-14 13:40:40           2837         67  1.0              \n","thedevastator/nike-usa-products-prices-descriptions-and-custom  Nike Products: Prices, Descriptions, Reviews         47KB  2022-11-28 07:01:00           1246         27  1.0              \n","piterfm/fifa-football-world-cup                                 FIFA Football World Cup                             179KB  2022-12-07 11:07:09           2290         63  1.0              \n","notshrirang/spotify-million-song-dataset                        Spotify Million Song Dataset                         21MB  2022-11-21 16:48:45           1574         36  1.0              \n","theakhilb/layoffs-data-2022                                     Layoffs Dataset 2022                                101KB  2022-12-08 12:03:06            752         25  1.0              \n","akshaydattatraykhare/diabetes-dataset                           Diabetes Dataset                                      9KB  2022-10-06 08:55:25          22389        582  1.0              \n","catherinerasgaitis/mxmh-survey-results                          Music & Mental Health Survey Results                 22KB  2022-11-21 10:03:12           1571         37  1.0              \n","malayvyas/usa-gdp-dataset-19612021                              USA GDP Growth Dataset 1961-2021                     10KB  2022-12-01 16:10:46            428         25  0.88235295       \n","sulphatet/daily-weather-data-40-years                           daily weather data: 40 years                          1MB  2022-11-30 17:51:22            392         25  0.7647059        \n","Downloading yelp-dataset.zip to /content\n","100% 4.07G/4.07G [02:16<00:00, 42.2MB/s]\n","100% 4.07G/4.07G [02:16<00:00, 31.9MB/s]\n","Archive:  yelp-dataset.zip\n","  inflating: Dataset_User_Agreement.pdf  \n","  inflating: yelp_academic_dataset_business.json  \n","  inflating: yelp_academic_dataset_checkin.json  \n","  inflating: yelp_academic_dataset_review.json  \n","  inflating: yelp_academic_dataset_tip.json  \n","  inflating: yelp_academic_dataset_user.json  \n"]}]},{"cell_type":"code","source":["def _get_data(): \n","    # read data from JSON file \n","    size = 150000\n","    review = pd.read_json('yelp_academic_dataset_review.json', lines=True,\n","                      dtype={'review_id':str,'user_id':str,\n","                             'business_id':str,'stars':int,\n","                             'date':str,'text':str,'useful':int,\n","                             'funny':int,'cool':int},\n","                      chunksize=size)\n","    \n","    #Gets reviews 100000-150000 \n","    lst = []\n","    for chunk_review in review:\n","      lst.append(chunk_review)\n","      break\n","    df_review = pd.concat(lst)\n","    df_review = df_review.iloc[100000:150000]\n","\n","    # extract review text and labels  \n","    dataset = df_review[['stars', 'text']].copy(deep=True)\n","    dataset['label'] = np.where(dataset['stars'] >= 4, 1, 0)\n","    \n","    # return dataset \n","    return dataset \n","\n","def get_clean_tokens(review): \n","    # tokenize review \n","    tokens = review.split() \n","\n","    # clean each token \n","    clean_tokens =[]\n","    for token in tokens: \n","        token = token.strip() \n","\n","        # remove non-alphaneumeric characters\n","        regex = re.compile('[^a-zA-Z0-9]')\n","        token = regex.sub('', token)\n","\n","        # record cleaned token\n","        if len(token) != 0: \n","            clean_tokens.append(token)\n","\n","    # return list of clean tokens \n","    return clean_tokens\n","    \n","def _tokenize_reviews(dataset): \n","    # get clean tokens of tweet\n","    dataset['text'] = dataset['text'].apply(\n","        lambda review: get_clean_tokens(review))\n","    \n","    # return dataset \n","    return dataset \n","\n","def _normalize_reviews(dataset): \n","    # normalize review text using stemming \n","    stemmer = SnowballStemmer('english')\n","    dataset['text'] = dataset['text'].apply(lambda tokens: \n","        [stemmer.stem(token) for token in tokens])\n","    \n","    # return dataset \n","    return dataset\n","\n","def _remove_stopwords_from_reviews(dataset):\n","    # remove stopwords\n","    nltk.download('stopwords')\n","    eng_stopwords = stopwords.words('english')\n","    dataset['text'] = dataset['text'].apply(lambda tokens: \n","        [token for token in tokens if token not in eng_stopwords])\n","    \n","    # return dataset \n","    return dataset \n","\n","def _vectorize_reviews(x_train, x_test): \n","    # vectorize using counts \n","    count_vectorizer = CountVectorizer(stop_words='english', \n","        max_features=10000)\n","    x_train_counts = count_vectorizer.fit_transform(x_train)\n","    x_test_counts = count_vectorizer.transform(x_test)\n","    \n","    # vectorize from counts using tf-idf \n","    tfidf_transformer = TfidfTransformer(norm='l2', sublinear_tf=True)\n","    x_train_tfidf = tfidf_transformer.fit_transform(x_train_counts)\n","    x_test_tfidf = tfidf_transformer.transform(x_test_counts)\n","    \n","    # return dataset \n","    return x_train_tfidf, x_test_tfidf, count_vectorizer, tfidf_transformer\n","\n","def _save_count_vectorizer(count_vectorizer): \n","    # save count vectorizer \n","    with open('./count_vectorizer.pkl', 'wb') as f: \n","        pickle.dump(count_vectorizer, f)\n","\n","def load_count_vectorizer(): \n","    # load count vectorizer \n","    with open('./count_vectorizer.pkl', 'rb') as f: \n","        count_vectorizer = pickle.load(f)\n","    \n","    # return count vectorizer \n","    return count_vectorizer\n","\n","def _save_tfidf_transformer(tfidf_transformer): \n","    # save count vectorizer \n","    with open('./tfidf_transformer.pkl', 'wb') as f: \n","        pickle.dump(tfidf_transformer, f)\n","\n","def load_tfidf_transformer(): \n","    # load count vectorizer \n","    with open('./tfidf_transformer.pkl', 'rb') as f: \n","        tfidf_transformer = pickle.load(f)\n","    \n","    # return count vectorizer \n","    return tfidf_transformer\n","\n","def process_data(holdout=0.2): \n","    # get data from file \n","    dataset = _get_data() \n","    \n","    # tokenize tweets\n","    dataset = _tokenize_reviews(dataset)\n","    \n","    # normalize tweet text \n","    dataset = _normalize_reviews(dataset)\n","    \n","    # remove stopwords \n","    dataset = _remove_stopwords_from_reviews(dataset)\n","    \n","    # join reviews, split data into train and test sets \n","    dataset['text'] = dataset['text'].apply(lambda tokens: ' '.join(tokens))\n","    x_train, x_test, y_train, y_test = train_test_split(dataset['text'], \n","        dataset['label'], test_size=holdout, shuffle=True) \n","    \n","    # vectorize features\n","    x_train, x_test, count_vectorizer, tfidf_transformer = _vectorize_reviews(\n","        x_train, x_test)\n","\n","    # save count vectorizer \n","    _save_count_vectorizer(count_vectorizer)\n","\n","    # save tfidf transformer \n","    _save_tfidf_transformer(tfidf_transformer)\n","    \n","    # return final dataset \n","    return x_train, x_test, y_train, y_test\n","\n","def transform_review(review): \n","    # load stemmer, vectorizers and stopwords \n","    try: \n","        eng_stopwords = stopwords.words('english')\n","    except: \n","        nltk.download('stopwords')\n","        eng_stopwords = stopwords.words('english')\n","    snowball_stemmer = SnowballStemmer(language='english')\n","    count_vectorizer = load_count_vectorizer()\n","    tfidf_transformer = load_tfidf_transformer() \n","\n","    # preprocess review \n","    tokens = get_clean_tokens(review)\n","    new_tokens = []\n","    for token in tokens: \n","        new_tokens.append(snowball_stemmer.stem(token)) \n","    tokens = new_tokens \n","    tokens = list(filter(lambda token: token not in eng_stopwords, tokens))\n","\n","    # transform review \n","    review_counts = count_vectorizer.transform(pd.DataFrame([review])[0])\n","    review_tfidf = tfidf_transformer.transform(review_counts)\n","\n","    # return transformed review \n","    return review_tfidf"],"metadata":{"id":"9OLC5EHBRVYG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def save_random_forest(random_forest): \n","    # save random forest model \n","    joblib.dump(random_forest, './random_forest.joblib')\n","\n","def load_random_forest(): \n","    # load random forest model \n","    random_forest = joblib.load('./random_forest.joblib')\n","\n","    # return loaded model \n","    return random_forest \n","\n","def fit_random_forest(x_train, x_test, y_train, y_test): \n","    # fit random forest model \n","    random_forest = RandomForestClassifier(n_estimators=10, max_depth=100)\n","    random_forest.fit(x_train, y_train)\n","\n","    # evaluate random forest model \n","    predictions = random_forest.predict(x_test)\n","    f1 = f1_score(y_test, predictions) \n","    accuracy = accuracy_score(y_test, predictions)\n","\n","    # save random forest model \n","    save_random_forest(random_forest)\n","\n","    # return model results \n","    return random_forest, f1, accuracy\n","\n","def predict_random_forest(review): \n","    # transform review \n","    review = transform_review(review)\n","\n","    # load random forest model \n","    random_forest = load_random_forest()\n","\n","    # predict sentiment of review \n","    predictions = random_forest.predict_proba(review)\n","\n","    # return prediction \n","    return predictions[0]\n","\n","def predict_multiple_random_forest(reviews, prob=False):\n","    # load stemmer, vectorizers, and model\n","    snowball_stemmer = SnowballStemmer(language='english')\n","    count_vectorizer = load_count_vectorizer()\n","    tfidf_transformer = load_tfidf_transformer() \n","    random_forest = load_random_forest() \n","\n","    # make predictions\n","    preds = [] \n","    pred_times = [] \n","    for review in reviews: \n","        start = time.time() \n","        # transform review\n","        try: \n","            eng_stopwords = stopwords.words('english')\n","        except: \n","            nltk.download('stopwords')\n","            eng_stopwords = stopwords.words('english')\n","        tokens = get_clean_tokens(review)\n","        new_tokens = []\n","        for token in tokens: \n","            new_tokens.append(snowball_stemmer.stem(token)) \n","        tokens = new_tokens \n","        tokens = list(filter(lambda token: token not in eng_stopwords, tokens))\n","        review_counts = count_vectorizer.transform(pd.DataFrame([review])[0])\n","        review_tfidf = tfidf_transformer.transform(review_counts)\n","\n","        # make prediction on review \n","        if prob: \n","            preds.append(random_forest.predict_proba(review_tfidf)[0])\n","        else: \n","            preds.append(random_forest.predict(review_tfidf)[0])\n","        end = time.time() \n","        pred_times.append(end - start)\n","\n","    # return predictions and stats\n","    mean = statistics.mean(pred_times)\n","    median = statistics.median(pred_times)\n","    range = max(pred_times) - min(pred_times)\n","    variance = statistics.variance(pred_times)\n","    stdev = statistics.stdev(pred_times)\n","    stats = [mean, median, range, variance, stdev]\n","    return preds, stats"],"metadata":{"id":"XaAyYE7EOw1I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# define constants\n","ITERATIONS = 5\n","\n","def test_data_processing_model_fitting(holdout): \n","    # initialize lists \n","    data_processing_times = [] \n","    random_forest_fit_times = [] \n","    random_forest_f1s = [] \n","    random_forest_accuracies = [] \n","\n","    # run iterations and record results\n","    print('Testing data processing time and model fitting')\n","    for _ in range(ITERATIONS):\n","\n","        # process data \n","        start = time.time()\n","        x_train, x_test, y_train, y_test = process_data(holdout) \n","        end = time.time()\n","        data_processing_times.append(end - start)\n","\n","        # fit random forest model \n","        start = time.time()\n","        _, f1, accuracy = fit_random_forest(x_train, x_test, y_train, y_test)\n","        end = time.time()\n","        random_forest_fit_times.append(end - start)\n","        random_forest_f1s.append(f1)\n","        random_forest_accuracies.append(accuracy)\n","\n","    # compute additional statistics for each list \n","    data = [\n","        data_processing_times, \n","        random_forest_fit_times, \n","        random_forest_f1s, \n","        random_forest_accuracies\n","    ] \n","    for i in range(len(data)): \n","        data_list = data[i]\n","        mean = statistics.mean(data_list)\n","        median = statistics.median(data_list)\n","        rng = max(data_list) - min(data_list)\n","        variance = statistics.variance(data_list)\n","        stdev = statistics.stdev(data_list)\n","        data_list += [mean, median, rng, variance, stdev]\n","        data[i] = data_list \n","\n","    # save result as CSV \n","    index = [\n","        'data processing time (s)',\n","        'random forest fit time (s)', \n","        'random forest f1',\n","        'random forest accuracy'\n","    ]\n","    columns = ['iteration_' + str(i + 1) for i in range(ITERATIONS)] + [\n","        'mean', 'median', 'range', 'variance', 'stdev']\n","    data_processing_model_fitting = pd.DataFrame(data, index, columns)\n","    data_processing_model_fitting.to_csv(\n","        f'./data_processing_model_fitting_holdout={holdout}.csv')\n","\n","def test_load_times(holdout):\n","    # test load times for vectorizers\n","    print('Testing load time for stemmer and vectorizers')\n","    stemmer_vectorizer_load_times = [] \n","    for _ in range(ITERATIONS):\n","        start = time.time() \n","        _ = SnowballStemmer(language='english')\n","        _ = load_count_vectorizer() \n","        _ = load_tfidf_transformer()\n","        end = time.time() \n","        stemmer_vectorizer_load_times.append(end - start)\n","    mean = statistics.mean(stemmer_vectorizer_load_times)\n","    median = statistics.median(stemmer_vectorizer_load_times)\n","    rng = max(stemmer_vectorizer_load_times) - min(\n","        stemmer_vectorizer_load_times)\n","    variance = statistics.variance(stemmer_vectorizer_load_times)\n","    stdev = statistics.stdev(stemmer_vectorizer_load_times)\n","    stemmer_vectorizer_load_times += [mean, median, rng, variance, stdev]\n","\n","    # test load times for random forest model \n","    print('Testing load time for random forest model')\n","    random_forest_load_times = [] \n","    for _ in range(ITERATIONS):\n","        start = time.time() \n","        _ = load_random_forest() \n","        end = time.time() \n","        random_forest_load_times.append(end - start)\n","    mean = statistics.mean(random_forest_load_times)\n","    median = statistics.median(random_forest_load_times)\n","    rng = max(random_forest_load_times) - min(random_forest_load_times)\n","    variance = statistics.variance(random_forest_load_times)\n","    stdev = statistics.stdev(random_forest_load_times)\n","    random_forest_load_times += [mean, median, rng, variance, stdev]\n","    \n","    # save result as CSV \n","    index = [\n","        'stemmer and vectorizers load time (s)', \n","        'random forest load time (s)'\n","    ]\n","    columns = ['iteration_' + str(i + 1) for i in range(ITERATIONS)] + [\n","        'mean', 'median', 'range', 'variance', 'stdev']\n","    data = [stemmer_vectorizer_load_times, random_forest_load_times]\n","    load_times = pd.DataFrame(data, index, columns)\n","    load_times.to_csv(f'./load_times_holdout={holdout}.csv')\n","\n","def test_predictions_reviews(holdout): \n","    # load data from csv file \n","    size = 35000\n","    reviews_df = pd.read_json('yelp_academic_dataset_review.json', lines=True,\n","                      dtype={'review_id':str,'user_id':str,\n","                             'business_id':str,'stars':int,\n","                             'date':str,'text':str,'useful':int,\n","                             'funny':int,'cool':int},\n","                      chunksize=size)\n","\n","    lst = []\n","    for chunk_review in reviews_df:\n","      lst.append(chunk_review)\n","      break\n","    df_review = pd.concat(lst)\n","    df_review['label'] = np.where(df_review['stars'] >= 4, 1, 0)\n","\n","    reviews = df_review['text']\n","    labels = df_review['label']\n","\n","    # make predictions with random forest model \n","    preds, stats = predict_multiple_random_forest(reviews)\n","    accuracy = accuracy_score(labels, preds)\n","    f1 = f1_score(labels, preds)\n","    random_forest_stats = [accuracy, f1] + stats\n","\n","    # save result as CSV \n","    index = [\n","        'random forest'\n","    ]\n","    columns = [\n","        'accuracy',\n","        'f1',\n","        'mean_prediction_time', \n","        'median_prediction_time', \n","        'range_prediction_time', \n","        'variance_prediction_time', \n","        'stdev_prediction_time'\n","    ]\n","    data = [random_forest_stats]\n","    load_times = pd.DataFrame(data, index, columns)\n","    load_times.to_csv(f'./prediction_stats_holdout={holdout}.csv')"],"metadata":{"id":"mIvREQ4_qlUp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# run experiments\n","holdouts = [0.4, 0.3, 0.2]\n","for holdout in holdouts: \n","    test_data_processing_model_fitting(holdout)\n","    test_load_times(holdout)\n","    test_predictions_reviews(holdout)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fcblxxNUtZnc","executionInfo":{"status":"ok","timestamp":1670534668497,"user_tz":300,"elapsed":1623218,"user":{"displayName":"Preethi Narayan","userId":"06220921649826715175"}},"outputId":"eb5e810f-faa3-4c59-8554-3101dd3325be"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Testing data processing time and model fitting\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["Testing load time for stemmer and vectorizers\n","Testing load time for random forest model\n","Testing data processing time and model fitting\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["Testing load time for stemmer and vectorizers\n","Testing load time for random forest model\n","Testing data processing time and model fitting\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["Testing load time for stemmer and vectorizers\n","Testing load time for random forest model\n"]}]},{"cell_type":"code","source":["def predict_sentiment(data):\n","\n","  rf_predictions_pos = []\n","  rf_predictions_neg = []\n","\n","  # load stemmer and vectorizers \n","  snowball_stemmer = SnowballStemmer(language='english')\n","  count_vectorizer = load_count_vectorizer()\n","  tfidf_transformer = load_tfidf_transformer() \n","  random_forest = load_random_forest() \n","\n","  # find predictions for each review\n","  for index, d in data.iterrows():\n","    # transform review \n","    review = d['text']\n","    try: \n","        eng_stopwords = stopwords.words('english')\n","    except: \n","        nltk.download('stopwords')\n","        eng_stopwords = stopwords.words('english')\n","    tokens = get_clean_tokens(review)\n","    new_tokens = []\n","    for token in tokens: \n","        new_tokens.append(snowball_stemmer.stem(token)) \n","    tokens = new_tokens \n","    tokens = list(filter(lambda token: token not in eng_stopwords, tokens))\n","    review_counts = count_vectorizer.transform(pd.DataFrame([review])[0])\n","    review_tfidf = tfidf_transformer.transform(review_counts)\n","\n","    rf_prediction = random_forest.predict_proba(review_tfidf)[0]\n","    \n","    rf_predictions_pos.append(rf_prediction[1])\n","    rf_predictions_neg.append(rf_prediction[0])\n","\n","  # append prediction probabilities into dataframe\n","  data[\"RF Positive\"] = rf_predictions_pos\n","  data[\"RF Negative\"] = rf_predictions_neg\n","\n","  data[\"Output\"] = np.where(data['RF Positive'] >= data[\"RF Negative\"], 1, 0)\n","\n","  # print percentage of positive reviews\n","  percent_positive = data[\"Output\"].mean()\n","  print(\"Percentage of positive reviews: \", percent_positive)\n","\n","  return data, percent_positive"],"metadata":{"id":"UV8clKrh7nJ_"},"execution_count":null,"outputs":[]}]}