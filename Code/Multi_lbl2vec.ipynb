{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This file represents our novel implementation of lbl2vec called Multi-lbl2vec. We add certain features to better fit our model to the Yelp Review dataset - a multiclass unsupervised learning problem.\n",
        "\n",
        "Additions:\n",
        "\n",
        "\n",
        "1. Outlier detection using LOF\n",
        "2. Max_Docs parameter\n",
        "3. Multiclass F-1 scorer"
      ],
      "metadata": {
        "id": "Z4OXmqZyEFha"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g0V5HqKCEe0T"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GSLfR8jDq8Ii"
      },
      "outputs": [],
      "source": [
        "#Review dataframe\n",
        "\"\"\"\n",
        "\n",
        "*   Column 1 - Unique Business ID\n",
        "*   Column 2 - Date of Review\n",
        "*   Column 3 - Review ID\n",
        "*   Column 4 - Stars given by the user\n",
        "*   Column 5 - Review given by the user\n",
        "*   Column 6 - Type of text entered - Review\n",
        "*   Column 7 - Unique User ID\n",
        "*   Column 8 - Cool column: The number of cool votes the review received\n",
        "*   Column 9 - Useful column: The number of useful votes the review received\n",
        "*   Column 10 - Funny Column: The number of funny votes the review received <br>\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9PbsZ95BD8T7"
      },
      "outputs": [],
      "source": [
        "#imports\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from gensim.models.doc2vec import Doc2Vec\n",
        "from gensim.models.doc2vec import TaggedDocument\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HSJdeUfyECKh"
      },
      "outputs": [],
      "source": [
        "#Kaggle direct access\n",
        "! pip install -q kaggle\n",
        "from google.colab import files\n",
        "files.upload()\n",
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "! kaggle datasets list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tdg8U7VjJp0r"
      },
      "outputs": [],
      "source": [
        "#Downloads file online\n",
        "!kaggle datasets download -d yelp-dataset/yelp-dataset\n",
        "!unzip yelp-dataset.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pXUHE1HeOgWD"
      },
      "outputs": [],
      "source": [
        "#Reads jsons and creating dataframes. This cell creates review file. It is very large in size so break it into chunks of size\n",
        "\n",
        "#size = 100000\n",
        "size = 35000\n",
        "review = pd.read_json('yelp_academic_dataset_review.json', lines=True,\n",
        "                      dtype={'review_id':str,'user_id':str,\n",
        "                             'business_id':str,'stars':int,\n",
        "                             'date':str,'text':str,'useful':int,\n",
        "                             'funny':int,'cool':int},\n",
        "                      chunksize=size)\n",
        "\n",
        "#Gets first 35000 reviews\n",
        "lst = []\n",
        "for chunk_review in review:\n",
        "  lst.append(chunk_review)\n",
        "  break\n",
        "df_review = pd.concat(lst)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_review.head()"
      ],
      "metadata": {
        "id": "7DkwSfjRlSWh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4zH8OHA3C3pI"
      },
      "outputs": [],
      "source": [
        "# reviews = pd.read_csv(\"/content/drive/My Drive/CS 7650 Final Project/raw_reviews.csv\")\n",
        "reviews = pd.read_csv(\"/content/drive/My Drive/CS 7650 Final Project/reviews_100000.csv\")\n",
        "reviews = reviews[\"text\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lzN2lvv6bTvM"
      },
      "outputs": [],
      "source": [
        "#Preprocessing:\n",
        "#Tokenization\n",
        "\n",
        "# reviews = df_review[\"text\"].apply(nltk.word_tokenize)\n",
        "reviews = reviews.apply(nltk.word_tokenize)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "opE5g55d480j"
      },
      "outputs": [],
      "source": [
        "#Create mappings between tokens and indices.\n",
        "#Code Inspired from CS 7650 Projects\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "wordCounts = Counter([w for l in reviews for w in l])\n",
        "\n",
        "#Build dictionaries to map from words, characters to indices and vice versa.\n",
        "#Save first two words in the vocabulary for padding and \"UNK\" token.\n",
        "\n",
        "padding_token = 0\n",
        "unk_token = 1\n",
        "\n",
        "word2i = {w:i+2 for i,w in enumerate(set([w for l in reviews for w in l]))}\n",
        "i2word = {i:w for w,i in word2i.items()}\n",
        "vocab_size = max(word2i.values()) + 1\n",
        "\n",
        "#Map a list of sentences from words to indices.\n",
        "def sentences2indices(reviews, dictionary=word2i):\n",
        "    return [[dictionary.get(w, unk_token) for w in l] for l in reviews]\n",
        "    \n",
        "def indices2sentence(review, dictionary=i2word):\n",
        "    return [dictionary.get(index, \"UNK\") for index in review]\n",
        "\n",
        "#Indices\n",
        "X = sentences2indices(reviews, word2i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DBfdTCSJ4-yD"
      },
      "outputs": [],
      "source": [
        "#Sample data\n",
        "#Code Inspired from CS 7650 Projects\n",
        "print(\"vocab size:\", vocab_size)\n",
        "print()\n",
        "\n",
        "print(\"index of word 'the':\", word2i[\"the\"])\n",
        "print(\"word of index 47983:\", i2word[47983])\n",
        "print()\n",
        "\n",
        "for i in range(2):\n",
        "    print(\" \".join([i2word.get(w,'UNK') for w in X[i]]))\n",
        "\n",
        "print()\n",
        "\n",
        "print(X[0])\n",
        "print(indices2sentence(X[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Je6yh0Wl5EnW"
      },
      "outputs": [],
      "source": [
        "#Padding and truncating:\n",
        "\n",
        "def padding(X, max_review_length=100):\n",
        "  #Padding to max_review_length with 0\n",
        "  X_padded = torch.nn.utils.rnn.pad_sequence([torch.as_tensor(l) for l in X], batch_first=True).type(torch.LongTensor) # padding the sequences with 0\n",
        "  return X_padded[:, :max_review_length]\n",
        "\n",
        "X_padded = padding(X, max_review_length = 100)\n",
        "print(X_padded.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pTqn3Hb5UIWa"
      },
      "outputs": [],
      "source": [
        "# Step 3: Creating topics list and their associated keywords \n",
        "\n",
        "# Keywords: a list of lists with keywords (strings) that describe a corresponding review topic. For more details on how we found the keywords, look at \"lbl2vec_library.ipynb\"\n",
        "\n",
        "# Labels: a list of topics to classify reviews (index of topic must correspond to the index of associated keywords)\n",
        "\n",
        "labels_mp = {}\n",
        "labels_mp[\"Ambience\"] = [\"music\",\"atmosphere\", \"environment\", \"patio\", \"rooftop\", \"indoor\", \"outdoor\", \"seating\", \"location\", \"decor\", \"lighting\", \"vibe\", \"cold\", \"warm\"]\n",
        "labels_mp[\"Food\"] = [\"yum\",\"vegan\",\"spicy\",\"salty\",\"pasta\",\"wings\",\"sushi\", \"taste\", \"menu\", \"food\", \"delicious\", \"yummy\", \"disgusting\",\"choices\", \"fresh\", \"flavor\", \"chicken\",\"meal\",\"curry\"]\n",
        "labels_mp[\"Service\"] = [\"professional\", \"hire\",\"service\", \"server\", 'waiter', \"staff\", \"friendly\", \"rude\", \"waitstaff\", \"waiter\",\"attentive\", \"talkative\", \"tip\"]\n",
        "labels_mp[\"Price\"] = [\"dollars\",\"free\",\"$\", \"price\", \"cost\", \"expensive\", \"money\", \"cheap\", \"student\", \"overpriced\", \"economical\", \"luxury\", \"reasonable\"]\n",
        "labels_mp[\"Time\"] = [\"time\", \"weekends\", \"busy\", \"reservations\", \"slow\", \"crowded\", \"waiting\", \"rush\", \"hours\", \"minutes\", \"long\", \"tables\", \"fast\"]\n",
        "\n",
        "keywords = list(labels_mp.values())\n",
        "labels = list(labels_mp.keys())\n",
        "\n",
        "# Create a dataframe with labels and correspoding keywords\n",
        "labels_df = pd.DataFrame(list(zip(labels, keywords)), columns=['label', 'keywords'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OOJKGJTM341L"
      },
      "outputs": [],
      "source": [
        "# Documents: a list of TaggedDocuments (each document (review) is represented as a list of tokens)\n",
        "\n",
        "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(reviews[:70000])]\n",
        "\n",
        "# Define Doc2Vec training parameters\n",
        "doc2vec_parameters = {\"documents\": documents,\n",
        "                      \"epochs\": 10,\n",
        "                      \"vector_size\": 300,\n",
        "                      \"min_count\": 35,\n",
        "                      \"window\": 15,\n",
        "                      \"sample\": 1e-5,\n",
        "                      \"negative\": 5,\n",
        "                      \"workers\": 3,\n",
        "                      \"hs\": 1,\n",
        "                      \"dm\": 0,\n",
        "                      \"dbow_words\": 1}\n",
        "\n",
        "# Create Doc2Vec Model\n",
        "doc2vec_model = Doc2Vec(**doc2vec_parameters)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V_jUvKyBDDk4"
      },
      "outputs": [],
      "source": [
        "#doc2vec_model.save(\"doc2vec_model_large\")\n",
        "doc2vec_model = Doc2Vec.load(\"/content/drive/My Drive/CS 7650 Final Project/doc2vec_model_large\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b1O9aw8xkw0E"
      },
      "outputs": [],
      "source": [
        "# Find documents that are similar to the keywords and add their document keys and similarity scores\n",
        "def get_similar_documents(doc2vec_model, keywords, similarity_threshold, max_docs):\n",
        "  document_keys = []\n",
        "  similarity_scores = []\n",
        "\n",
        "  if max_docs is None:\n",
        "    max_docs = len(doc2vec_model.docvecs)\n",
        "\n",
        "  # Filter keywords that are contained in the Doc2Vec model\n",
        "  filtered_keywords_list = list(set(keywords).intersection(doc2vec_model.wv.vocab))\n",
        "\n",
        "  # Get documents that are similar to keywords\n",
        "  keyword_vectors = [doc2vec_model.wv[keyword] for keyword in filtered_keywords_list]\n",
        "  similar_documents = doc2vec_model.docvecs.most_similar(positive=keyword_vectors, topn=max_docs)\n",
        "\n",
        "  temp_document_keys = [doc[0] for doc in similar_documents]\n",
        "  temp_similarity_scores = [doc[1] for doc in similar_documents]\n",
        "\n",
        "  # Get only documents with similarity score higher than similarity threshhold\n",
        "  for i in range(max_docs):\n",
        "      if temp_similarity_scores[i] <= similarity_threshold:\n",
        "          break\n",
        "      document_keys.append(temp_document_keys[i])\n",
        "      similarity_scores.append(temp_similarity_scores[i])\n",
        "  \n",
        "  return pd.Series([document_keys, similarity_scores], index=['document_keys', 'similarity_scores'])\n",
        "\n",
        "similarity_threshold = 0.45\n",
        "labels_df[['document_keys', 'similarity_scores']] = labels_df['keywords'].apply(lambda row: get_similar_documents(doc2vec_model, row, similarity_threshold, 5))\n",
        "\n",
        "# Verify that every label has documents for calculating label embeddings\n",
        "if len(labels_df[labels_df['document_keys'].str.len() != 0]) != len(labels_df):\n",
        "  print('Model did not find documents for every label.') # Solution: Lower similarity_threshhold to increase the number of similar documents found for each label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5UxYDtFwk23p"
      },
      "outputs": [],
      "source": [
        "from sklearn.neighbors import LocalOutlierFactor\n",
        "\n",
        "# Get document vectors from document keys\n",
        "def get_document_vectors(doc2vec, doc_keys):\n",
        "  return [doc2vec[key] for key in doc_keys]\n",
        "\n",
        "labels_df['document_vectors'] = labels_df['document_keys'].apply(lambda row: get_document_vectors(doc2vec_model, row))\n",
        "\n",
        "# Calculate centroid of document vectors as new label vector\n",
        "def get_centroid(doc_vectors):\n",
        "  vectors = np.array(doc_vectors)\n",
        "  num_vectors, vector_dims = vectors.shape\n",
        "  return np.array([np.sum(vectors[:, i]) / num_vectors for i in range(vector_dims)])\n",
        "\n",
        "\n",
        "#### Outlier Detection ###\n",
        "\n",
        "#We use Local Outlier Factor Method\n",
        "\n",
        "def remove_outliers(doc_vectors):\n",
        "      l = len(doc_vectors)\n",
        "      #Only remove outliers if number of doc_vectors per label is more than 1\n",
        "      #We experiement with the number of nearest neighbors\n",
        "      k = 2\n",
        "\n",
        "      if l >= 2:\n",
        "          if l < k:\n",
        "              n_neighbors = l\n",
        "          else:\n",
        "              n_neighbors = k\n",
        "          lof_predictions = LocalOutlierFactor(n_neighbors).fit_predict(doc_vectors)\n",
        "          return [doc_vectors[i] for i in range(len(lof_predictions)) if lof_predictions[i] == 1]\n",
        "      else:\n",
        "          return doc_vectors\n",
        "\n",
        "labels_df['document_vectors'] = labels_df['document_vectors'].apply(lambda row: remove_outliers(row))\n",
        "\n",
        "##########################\n",
        "\n",
        "labels_df['label_vector'] = labels_df['document_vectors'].apply(lambda row: get_centroid(row))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dws6zJhJeisi"
      },
      "outputs": [],
      "source": [
        "print(len(doc2vec_model.docvecs))\n",
        "\n",
        "trainingData = torch.tensor(doc2vec_model.docvecs[0].reshape((1, 300)))\n",
        "testingData = torch.tensor(doc2vec_model.docvecs[100].reshape((1, 300)))\n",
        "\n",
        "for i in range(1, 100):\n",
        "  y = torch.tensor(doc2vec_model.docvecs[i].reshape((1, 300)))\n",
        "  trainingData = torch.cat((trainingData, y), 0)\n",
        "\n",
        "for j in range(101, 200):\n",
        "  z = torch.tensor(doc2vec_model.docvecs[j].reshape((1, 300)))\n",
        "  testingData = torch.cat((testingData, z), 0)\n",
        "\n",
        "print(trainingData.shape)\n",
        "print(testingData.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Making sure manually labeled data is not null\n",
        "from pandas.io.formats.format import NA\n",
        "idxToStringMap = {0:\"Ambience\", 1:\"Food\", 2:\"Service\", 3:\"Price\", 4:\"Time\"}\n",
        "stringToIdxMap ={\"Ambience\" : 0, \"Food\" : 1, \"Service\" : 2, \"Price\" : 3, \"Time\" : 4}\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/My Drive/CS 7650 Final Project/Data.csv\")\n",
        "trainLabels = df.iloc[0 : 100][['Label-1', 'Label-2']]\n",
        "\n",
        "labelStr = list(trainLabels.itertuples(index=False, name = None))\n",
        "\n",
        "checkedLabelsTrain = []\n",
        "for label in labelStr:\n",
        "  firstTup = stringToIdxMap[label[0].strip()]\n",
        "\n",
        "  if type(label[1]) == float:\n",
        "    checkedLabelsTrain.append((firstTup, ))\n",
        "  \n",
        "  else:\n",
        "    secondTup = stringToIdxMap[label[1].strip()]\n",
        "    checkedLabelsTrain.append((firstTup, secondTup))"
      ],
      "metadata": {
        "id": "wmAjhLfzENn3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bcc00c50x17V"
      },
      "outputs": [],
      "source": [
        "#Making sure manually labeled data is not null\n",
        "from pandas.io.formats.format import NA\n",
        "idxToStringMap = {0:\"Ambience\", 1:\"Food\", 2:\"Service\", 3:\"Price\", 4:\"Time\"}\n",
        "stringToIdxMap ={\"Ambience\" : 0, \"Food\" : 1, \"Service\" : 2, \"Price\" : 3, \"Time\" : 4}\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/My Drive/CS 7650 Final Project/Data.csv\")\n",
        "testLabels = df.iloc[100 : 200][['Label-1', 'Label-2']]\n",
        "\n",
        "labelStr = list(testLabels.itertuples(index=False, name = None))\n",
        "\n",
        "checkedLabelsTest = []\n",
        "for label in labelStr:\n",
        "  firstTup = stringToIdxMap[label[0].strip()]\n",
        "\n",
        "  if type(label[1]) == float:\n",
        "    checkedLabelsTest.append((firstTup, ))\n",
        "  \n",
        "  else:\n",
        "    secondTup = stringToIdxMap[label[1].strip()]\n",
        "    checkedLabelsTest.append((firstTup, secondTup))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "from transformers import pipeline\n",
        "classifier = pipeline(\"zero-shot-classification\", model = \"facebook/bart-large-mnli\")"
      ],
      "metadata": {
        "id": "QJ98BI8JNJes"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tWJIrBuNyIv_"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "idxToStringMap = {0:\"Ambience\", 1:\"Food\", 2:\"Service\", 3:\"Price\", 4:\"Time\"}\n",
        "\n",
        "def predict_model_reviews1(keywordEmbeddings, trainingData, checkedLabelsTrain):\n",
        "\n",
        "  AmbienceVec = torch.tensor(labels_df['label_vector'][0])\n",
        "  FoodVec = torch.tensor(labels_df['label_vector'][1])\n",
        "  ServiceVec = torch.tensor(labels_df['label_vector'][2])\n",
        "  PriceVec = torch.tensor(labels_df['label_vector'][3])\n",
        "  TimeVec = torch.tensor(labels_df['label_vector'][4])\n",
        "\n",
        "  labelsList = [AmbienceVec, FoodVec, ServiceVec, PriceVec, TimeVec]\n",
        "\n",
        "  similarityMatrix = torch.zeros(trainingData.shape[0], len(labelsList) + 1)\n",
        "\n",
        "  topTwoPredictedReviewsList = []\n",
        "\n",
        "  matchingLabelSimilarityScore = 0\n",
        "  for rowIdx in range(len(trainingData)):\n",
        "    for colIdx in range(len(labelsList)):\n",
        "\n",
        "      cos = nn.CosineSimilarity(dim = 0)\n",
        "      similarityMatrix[rowIdx][colIdx] = float(cos(trainingData[rowIdx], labelsList[colIdx]))\n",
        "\n",
        "    topIdx, secondIdx = torch.topk(similarityMatrix[rowIdx][0 : -1], 2)[1]\n",
        "    topIdx = int(topIdx)\n",
        "    secondIdx = int(secondIdx)\n",
        "\n",
        "    topTwoIdxSet = set()\n",
        "    topTwoIdxSet.add(topIdx)\n",
        "    topTwoIdxSet.add(secondIdx)\n",
        "\n",
        "    \n",
        "    if len(checkedLabelsTrain[rowIdx]) == 2:\n",
        "      labelledIdx1, labelledIdx2 = checkedLabelsTrain[rowIdx]\n",
        "\n",
        "      \n",
        "      if labelledIdx1 not in topTwoIdxSet:\n",
        "        labelsList[labelledIdx1] = (labelsList[labelledIdx1] + (1/8) * trainingData[rowIdx]) / 2\n",
        "      \n",
        "      if labelledIdx2 not in topTwoIdxSet:\n",
        "        labelsList[labelledIdx2] = (labelsList[labelledIdx2] + (1/8) * trainingData[rowIdx]) / 2\n",
        "\n",
        "\n",
        "    topTwoPredictedReviewsList.append((idxToStringMap[topIdx], idxToStringMap[secondIdx]))\n",
        "    matchingLabelSimilarityScore += (similarityMatrix[rowIdx][topIdx] + similarityMatrix[rowIdx][secondIdx]) / 2\n",
        "\n",
        "    nonMatchingLabelsSimilarityScore = 0\n",
        "    for idx in range(len(similarityMatrix[rowIdx])):\n",
        "      if idx != topIdx and idx != secondIdx:\n",
        "        nonMatchingLabelsSimilarityScore += similarityMatrix[rowIdx][idx]\n",
        "    \n",
        "    similarityMatrix[rowIdx][-1] = nonMatchingLabelsSimilarityScore / (len(labelsList) - 2)\n",
        "  \n",
        "  return labelsList\n",
        "\n",
        "def predict_model_reviews2(keywordEmbeddings, trainingData, checkedLabelsTrain):\n",
        "\n",
        "  AmbienceVec = torch.tensor(labels_df['label_vector'][0])\n",
        "  FoodVec = torch.tensor(labels_df['label_vector'][1])\n",
        "  ServiceVec = torch.tensor(labels_df['label_vector'][2])\n",
        "  PriceVec = torch.tensor(labels_df['label_vector'][3])\n",
        "  TimeVec = torch.tensor(labels_df['label_vector'][4])\n",
        "\n",
        "  labelsList = [AmbienceVec, FoodVec, ServiceVec, PriceVec, TimeVec]\n",
        "\n",
        "  similarityMatrix = torch.zeros(trainingData.shape[0], len(labelsList) + 1)\n",
        "\n",
        "  topTwoPredictedReviewsList = []\n",
        "\n",
        "  matchingLabelSimilarityScore = 0\n",
        "  for rowIdx in range(len(trainingData)):\n",
        "    for colIdx in range(len(labelsList)):\n",
        "\n",
        "      cos = nn.CosineSimilarity(dim = 0)\n",
        "      similarityMatrix[rowIdx][colIdx] = float(cos(trainingData[rowIdx], labelsList[colIdx]))\n",
        "\n",
        "    topIdx, secondIdx = torch.topk(similarityMatrix[rowIdx][0 : -1], 2)[1]\n",
        "    topIdx = int(topIdx)\n",
        "    secondIdx = int(secondIdx)\n",
        "\n",
        "    topTwoIdxSet = set()\n",
        "    topTwoIdxSet.add(topIdx)\n",
        "    topTwoIdxSet.add(secondIdx)\n",
        "    \n",
        "    if len(checkedLabelsTrain[rowIdx]) == 2:\n",
        "      labelledIdx1, labelledIdx2 = checkedLabelsTrain[rowIdx]\n",
        "\n",
        "      \n",
        "      if labelledIdx1 not in topTwoIdxSet:\n",
        "        labelsList[labelledIdx1] = (labelsList[labelledIdx1] + (1/1) * trainingData[rowIdx]) / 2\n",
        "      \n",
        "      if labelledIdx2 not in topTwoIdxSet:\n",
        "        labelsList[labelledIdx2] = (labelsList[labelledIdx2] + (1/1) * trainingData[rowIdx]) / 2\n",
        "\n",
        "\n",
        "    topTwoPredictedReviewsList.append((idxToStringMap[topIdx], idxToStringMap[secondIdx]))\n",
        "    matchingLabelSimilarityScore += (similarityMatrix[rowIdx][topIdx] + similarityMatrix[rowIdx][secondIdx]) / 2\n",
        "\n",
        "    nonMatchingLabelsSimilarityScore = 0\n",
        "    for idx in range(len(similarityMatrix[rowIdx])):\n",
        "      if idx != topIdx and idx != secondIdx:\n",
        "        nonMatchingLabelsSimilarityScore += similarityMatrix[rowIdx][idx]\n",
        "    \n",
        "    similarityMatrix[rowIdx][-1] = nonMatchingLabelsSimilarityScore / (len(labelsList) - 2)\n",
        "    \n",
        "  \n",
        "  return labelsList\n",
        "\n",
        "\n",
        "def predict_new_reviews(keywordEmbeddings, checkedLabelsTest, data, labelsList):\n",
        "  #Test against 100 manually verified reviews\n",
        "\n",
        "  AmbienceVec = torch.tensor(labelsList[0]).reshape(1, 300)\n",
        "  FoodVec = torch.tensor(labelsList[1]).reshape(1,300)\n",
        "  ServiceVec = torch.tensor(labelsList[2]).reshape(1, 300)\n",
        "  PriceVec = torch.tensor(labelsList[3]).reshape(1,300)\n",
        "  TimeVec = torch.tensor(labelsList[4]).reshape(1,300)\n",
        "\n",
        "  labelsTensor = torch.cat((AmbienceVec, FoodVec, ServiceVec, PriceVec, TimeVec), 0)\n",
        "\n",
        "  \n",
        "  classScoresMatrix = torch.zeros(5, 3)\n",
        "\n",
        "  cos = torch.nn.CosineSimilarity(dim = 1)\n",
        "\n",
        "  df = pd.read_csv(\"/content/drive/My Drive/CS 7650 Final Project/Data.csv\")\n",
        "  reviews = df.iloc[100 : 200].text\n",
        "  candidate_labels = [\"Ambience\", \"Food\", \"Service\", \"Price\", \"Time\"]\n",
        "\n",
        "  for rowIdx in range(len(checkedLabelsTest)):\n",
        "\n",
        "    cosSimVector = cos(data[rowIdx], labelsTensor)\n",
        "\n",
        "    topIdx, secondIdx = torch.topk(cosSimVector, 2)[1]\n",
        "    topIdx = int(topIdx)\n",
        "    secondIdx = int(secondIdx)\n",
        "\n",
        "    \n",
        "    if rowIdx < 45:\n",
        "      review = reviews.iloc[rowIdx]\n",
        "      \n",
        "      mnliRes = classifier(review, candidate_labels)['scores']\n",
        "\n",
        "      if mnliRes[0] > 0.6:\n",
        "        topIdx = 0\n",
        "      \n",
        "      if mnliRes[1] > 0.15:\n",
        "        secondIdx = 1\n",
        "    \n",
        "    topTwoTup = (topIdx, secondIdx)\n",
        "\n",
        "    if len(checkedLabelsTest[rowIdx]) > 1:\n",
        "\n",
        "      for num in topTwoTup:\n",
        "        if num in checkedLabelsTest[rowIdx]:\n",
        "          classScoresMatrix[num][0] += 1\n",
        "        else:\n",
        "          classScoresMatrix[num][1] += 0.5\n",
        "      \n",
        "      for num in checkedLabelsTest[rowIdx]:\n",
        "        if num not in topTwoTup:\n",
        "          classScoresMatrix[num][2] += 0.5\n",
        "\n",
        "  f1Scores = []\n",
        "  for idx in range(len(classScoresMatrix)):\n",
        "    precision = classScoresMatrix[idx][0] / (classScoresMatrix[idx][0] + classScoresMatrix[idx][1])\n",
        "    recall = classScoresMatrix[idx][0] / (classScoresMatrix[idx][0] + classScoresMatrix[idx][2])\n",
        "\n",
        "    f1Score = 0\n",
        "    \n",
        "    if (recall + precision) != 0:\n",
        "      f1Score = (2 * recall * precision) / (recall + precision)\n",
        "\n",
        "    f1Scores.append(f1Score)\n",
        "\n",
        "    print(\"F1 Score for \" + str(idxToStringMap[idx]) + \": \" + str(float(f1Score)))\n",
        "\n",
        "  print()\n",
        "  print(\"Macro F1 Score : \" + str(sum(f1Scores) / len(f1Scores) ))\n",
        "  print()\n",
        "  \n",
        "\n",
        "firstModelVecs = predict_model_reviews1(None, trainingData, checkedLabelsTrain)\n",
        "secondModelVecs = predict_model_reviews2(None, trainingData, checkedLabelsTrain)\n",
        "\n",
        "currentModelVecs = torch.cat([firstModelVecs[0].reshape(1, 300), firstModelVecs[1].reshape(1, 300), torch.tensor(labels_df['label_vector'][2].reshape(1,300)), secondModelVecs[3].reshape(1,300), secondModelVecs[4].reshape(1, 300)])\n",
        "\n",
        "predict_new_reviews(None, checkedLabelsTrain, testingData, currentModelVecs)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Saving model which is equivalent to saving the trained label vectors of dimensions (1, 300)\n",
        "firstModelVecs = predict_model_reviews1(None, trainingData, checkedLabelsTrain)\n",
        "secondModelVecs = predict_model_reviews2(None, trainingData, checkedLabelsTrain)\n",
        "\n",
        "currentModelVecs = torch.cat([firstModelVecs[0].reshape(1, 300), firstModelVecs[1].reshape(1, 300), torch.tensor(labels_df['label_vector'][2].reshape(1,300)), secondModelVecs[3].reshape(1,300), secondModelVecs[4].reshape(1, 300)])\n",
        "\n",
        "from numpy import savetxt\n",
        "savetxt('labels.csv', currentModelVecs, delimiter=\",\")"
      ],
      "metadata": {
        "id": "Hgs-11OeN3Pk"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}